I"9,<blockquote>
  <p>本文主要记录一下 Kubernetes 使用 Ceph 存储的相关配置过程，Kubernetes 集群环境采用的 kargo 部署方式，并且所有组件以容器化运行</p>
</blockquote>

<h3 id="一基础环境准备">一、基础环境准备</h3>

<p>Kubernetes 集群总共有 5 台，部署方式为 kargo 容器化部署，<strong>采用 kargo 部署时确保配置中开启内核模块加载( <code class="highlighter-rouge">kubelet_load_modules: true</code> )</strong>；Kubernetes 版本为 1.6.4，Ceph 采用最新的稳定版 Jewel</p>

<table>
  <thead>
    <tr>
      <th>节点</th>
      <th>IP</th>
      <th>部署</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>docker1</td>
      <td>192.168.1.11</td>
      <td>master、monitor、osd</td>
    </tr>
    <tr>
      <td>docker2</td>
      <td>192.168.1.12</td>
      <td>master、monitor、osd</td>
    </tr>
    <tr>
      <td>docker3</td>
      <td>192.168.1.13</td>
      <td>node、monitor、osd</td>
    </tr>
    <tr>
      <td>docker4</td>
      <td>192.168.1.14</td>
      <td>node、osd</td>
    </tr>
    <tr>
      <td>docker5</td>
      <td>192.168.1.15</td>
      <td>node、osd</td>
    </tr>
  </tbody>
</table>

<h3 id="二部署-ceph-集群">二、部署 Ceph 集群</h3>

<p>具体安装请参考 <a href="https://mritd.me/2017/05/27/ceph-note-1/">Ceph 笔记(一)</a>、<a href="https://mritd.me/2017/05/30/ceph-note-2/">Ceph 笔记(二)</a>，以下直接上命令</p>

<h4 id="21部署集群">2.1、部署集群</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建集群配置目录</span>
<span class="nb">mkdir </span>ceph-cluster <span class="o">&amp;&amp;</span> <span class="nb">cd </span>ceph-cluster
<span class="c"># 创建 monitor-node</span>
ceph-deploy new docker1 docker2 docker3
<span class="c"># 追加 OSD 副本数量</span>
<span class="nb">echo</span> <span class="s2">"osd pool default size = 5"</span> <span class="o">&gt;&gt;</span> ceph.conf
<span class="c"># 安装 ceph</span>
ceph-deploy <span class="nb">install </span>docker1 docker2 docker3 docker4 docker5
<span class="c"># init monitor node</span>
ceph-deploy mon create-initial
<span class="c"># 初始化 ods</span>
ceph-deploy osd prepare docker1:/dev/sda docker2:/dev/sda docker3:/dev/sda docker4:/dev/sda docker5:/dev/sda
<span class="c"># 激活 osd</span>
ceph-deploy osd activate docker1:/dev/sda1:/dev/sda2 docker2:/dev/sda1:/dev/sda2 docker3:/dev/sda1:/dev/sda2 docker4:/dev/sda1:/dev/sda2 docker5:/dev/sda1:/dev/sda2
<span class="c"># 部署 ceph cli 工具和秘钥文件</span>
ceph-deploy admin docker1 docker2 docker3 docker4 docker5
<span class="c"># 确保秘钥有读取权限</span>
<span class="nb">chmod</span> +r /etc/ceph/ceph.client.admin.keyring
<span class="c"># 检测集群状态</span>
ceph health
</code></pre></div></div>

<h4 id="22创建块设备">2.2、创建块设备</h4>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建存储池</span>
rados mkpool data
<span class="c"># 创建 image</span>
rbd create data <span class="nt">--size</span> 10240 <span class="nt">-p</span> data
<span class="c"># 关闭不支持特性</span>
rbd feature disable data exclusive-lock, object-map, fast-diff, deep-flatten <span class="nt">-p</span> data
<span class="c"># 映射(每个节点都要映射)</span>
rbd map data <span class="nt">--name</span> client.admin <span class="nt">-p</span> data
<span class="c"># 格式化块设备(单节点即可)</span>
mkfs.xfs /dev/rbd0
</code></pre></div></div>

<h3 id="三kubernetes-使用-ceph">三、kubernetes 使用 Ceph</h3>

<h4 id="31pv--pvc-方式">3.1、PV &amp; PVC 方式</h4>

<p>传统的使用分布式存储的方案一般为 <code class="highlighter-rouge">PV &amp; PVC</code> 方式，也就是说管理员预先创建好相关 PV 和 PVC，然后对应的 deployment 或者 replication 挂载 PVC 来使用</p>

<p><strong>创建 Secret</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 获取管理 key 并进行 base64 编码</span>
ceph auth get-key client.admin | <span class="nb">base64</span>

<span class="c"># 创建一个 secret 配置(key 为上条命令生成的)</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt;&gt; ceph-secret.yml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: QVFDaWtERlpzODcwQWhBQTdxMWRGODBWOFZxMWNGNnZtNmJHVGc9PQo=
</span><span class="no">EOF
</span>kubectl create <span class="nt">-f</span> ceph-secret.yml
</code></pre></div></div>

<p><strong>创建 PV</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># monitor 需要多个，pool 和 image 填写上面创建的</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt;&gt; test.pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce 
  rbd:
    monitors:
      - 192.168.1.11:6789
      - 192.168.1.12:6789
      - 192.168.1.13:6789
    pool: data
    image: data
    user: admin
    secretRef:
      name: ceph-secret
    fsType: xfs
    readOnly: false
  persistentVolumeReclaimPolicy: Recycle
</span><span class="no">EOF

</span>kubectl create <span class="nt">-f</span> test.pv.yml
</code></pre></div></div>

<p><strong>创建 PVC</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt;&gt; test.pvc.yml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
</span><span class="no">EOF

</span>kubectl create <span class="nt">-f</span> test.pvc.yml
</code></pre></div></div>

<p><strong>创建 Deployment并挂载</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt;&gt; test.deploy.yml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: demo
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
      - name: demo
        image: mritd/demo
        ports:
        - containerPort: 80
        volumeMounts:
          - mountPath: "/data"
            name: data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: test-pvc
</span><span class="no">EOF

</span>kubectl create <span class="nt">-f</span> test.deploy.yml
</code></pre></div></div>

<h4 id="32storagaclass-方式">3.2、StoragaClass 方式</h4>

<p>在 1.4 以后，kubernetes 提供了一种更加方便的动态创建 PV 的方式；也就是说使用 StoragaClass 时无需预先创建固定大小的 PV，等待使用者创建 PVC 来使用；而是直接创建 PVC 即可分配使用</p>

<p><strong>创建系统级 Secret</strong></p>

<p><strong>注意: 由于 StorageClass 要求 Ceph 的 Secret type 必须为 <code class="highlighter-rouge">kubernetes.io/rbd</code>，所以上一步创建的 <code class="highlighter-rouge">ceph-secret</code> 需要先被删除，然后使用如下命令重新创建；此时的 key 并没有经过 base64</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 这个 secret type 必须为 kubernetes.io/rbd，否则会造成 PVC 无法使用</span>
kubectl create secret generic ceph-secret <span class="nt">--type</span><span class="o">=</span><span class="s2">"kubernetes.io/rbd"</span> <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">key</span><span class="o">=</span><span class="s1">'AQCikDFZs870AhAA7q1dF80V8Vq1cF6vm6bGTg=='</span> <span class="nt">--namespace</span><span class="o">=</span>kube-system
kubectl create secret generic ceph-secret <span class="nt">--type</span><span class="o">=</span><span class="s2">"kubernetes.io/rbd"</span> <span class="nt">--from-literal</span><span class="o">=</span><span class="nv">key</span><span class="o">=</span><span class="s1">'AQCikDFZs870AhAA7q1dF80V8Vq1cF6vm6bGTg=='</span> <span class="nt">--namespace</span><span class="o">=</span>default
</code></pre></div></div>

<p><strong>创建 StorageClass</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt;&gt; test.storageclass.yml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: test-storageclass
provisioner: kubernetes.io/rbd
parameters:
  monitors: 192.168.1.11:6789,192.168.1.12:6789,192.168.1.13:6789
  # Ceph 客户端用户 ID(非 k8s 的)
  adminId: admin
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  pool: data
  userId: admin
  userSecretName: ceph-secret
</span><span class="no">EOF

</span>kubectl create <span class="nt">-f</span> test.storageclass.yml
</code></pre></div></div>

<p><strong>关于上面的 adminId 等字段具体含义请参考这里 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#ceph-rbd">Ceph RBD</a></strong></p>

<p><strong>创建 PVC</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt;&gt; test.sc.pvc.yml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-sc-pvc
  annotations: 
    volume.beta.kubernetes.io/storage-class: test-storageclass
spec:
  accessModes:
    - ReadWriteOnce 
  resources:
    requests:
      storage: 2Gi
</span><span class="no">EOF

</span>kubectl create <span class="nt">-f</span> test.sc.pvc.yml
</code></pre></div></div>

<p><strong>创建 Deployment</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> &gt;&gt; test.sc.deploy.yml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: demo-sc
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: demo-sc
    spec:
      containers:
      - name: demo-sc
        image: mritd/demo
        ports:
        - containerPort: 80
        volumeMounts:
          - mountPath: "/data"
            name: data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: test-sc-pvc
</span><span class="no">EOF

</span>kubectl create <span class="nt">-f</span> test.sc.deploy.yml
</code></pre></div></div>

<p>到此完成，检测是否成功最简单的方式就是看相关 pod 是否正常运行</p>

<p>转载请注明出处，本文采用 <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/">CC4.0</a> 协议授权</p>
:ET